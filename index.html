<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Modal Movie Genre Classification</title>
    <style>
        :root {
            --primary-color: #3498db;
            --secondary-color: #2c3e50;
            --accent-color: #e74c3c;
            --light-bg: #f8f9fa;
            --dark-text: #333;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--dark-text);
            background-color: var(--light-bg);
        }
        
        header {
            background: linear-gradient(to right, var(--primary-color), var(--secondary-color));
            color: white;
            padding: 2rem 0;
            text-align: center;
        }
        
        .container {
            width: 90%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem 0;
        }
        
        nav {
            background-color: var(--secondary-color);
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 1000;
        }
        
        nav ul {
            display: flex;
            justify-content: center;
            list-style-type: none;
        }
        
        nav ul li {
            margin: 0 1rem;
        }
        
        nav ul li a {
            color: white;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        
        nav ul li a:hover {
            color: var(--primary-color);
        }
        
        section {
            margin: 2rem 0;
            padding: 2rem;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        h1, h2, h3 {
            color: var(--secondary-color);
            margin-bottom: 1rem;
        }
        
        p {
            margin-bottom: 1rem;
        }
        
        .team-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }
        
        .team-member {
            background-color: var(--light-bg);
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }
        
        .chart-container {
            background-color: white;
            padding: 1rem;
            border-radius: 8px;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }
        
        .model-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }
        
        .model-card {
            background-color: var(--light-bg);
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }
        
        .model-card h3 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 0.5rem;
            margin-bottom: 1rem;
        }
        
        .btn {
            display: inline-block;
            background-color: var(--primary-color);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease;
        }
        
        .btn:hover {
            background-color: var(--secondary-color);
        }
        
        footer {
            background-color: var(--secondary-color);
            color: white;
            text-align: center;
            padding: 1.5rem 0;
            margin-top: 3rem;
        }
        
        .chart-row {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            gap: 20px;
            margin-bottom: 30px;
        }
        
        .chart-item {
            flex: 1;
            min-width: 300px;
        }
        
        .code-block {
            background-color: #f5f5f5;
            padding: 1rem;
            border-radius: 4px;
            margin: 1rem 0;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        
        table, th, td {
            border: 1px solid #ddd;
        }
        
        th, td {
            padding: 0.75rem;
            text-align: left;
        }
        
        th {
            background-color: var(--light-bg);
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Multi-Modal Movie Genre Classification</h1>
            <p>A Comparative Study of Text-Based, Image-Based, and Fusion Approaches</p>
        </div>
    </header>
    
    <nav>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#methodology">Methodology</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#team">Team</a></li>
            <li><a href="#future">Future Work</a></li>
        </ul>
    </nav>
    
    <div class="container">
        <section id="introduction">
            <h2>1. Introduction & Objectives</h2>
            <p>Our project addresses the challenge of movie genre classification through a comparative study of three distinct approaches: text-based, image-based, and a multimodal fusion of both with tabular data. The primary objective was to evaluate how different data modalities contribute to classification accuracy and determine whether an integrated approach yields superior performance over single-modality methods.</p>
            
            <p>This work tackles multi-label classification, where movies typically belong to multiple genre categories simultaneously, making it a particularly challenging problem in the field of content categorization.</p>
            
            <div class="chart-container">
                <img src="/api/placeholder/800/400" alt="Project Overview Diagram" />
                <p class="caption">Figure 1: Overview of our multi-modal approach combining text, image, and tabular data for movie genre classification.</p>
            </div>
        </section>
        
        <section id="dataset">
            <h2>2. Dataset Description</h2>
            <p>Our analysis utilized a comprehensive movie dataset from TMDB containing rich metadata including titles, overviews, release dates, vote averages, and genre information, accompanied by corresponding poster images. Extensive preprocessing was required to prepare this raw data for analysis.</p>
            
            <h3>Dataset Challenges & Solutions</h3>
            <p>We implemented a MultiLabelBinarizer to transform genre lists into one-hot encoded vectors, addressing the initial challenge of multi-class genre assignments. To combat severe class imbalance, where certain genres like Drama and Comedy significantly outnumbered others like TV Movie and Western, we developed a strategic genre grouping system that consolidated the original 19 genres into five balanced thematic clusters, creating a more equitable distribution for machine learning applications.</p>
            
            <div class="chart-row">
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="image1.png" alt="Original Genre Distribution" />
                        <p class="caption">Figure 2: Original genre distribution showing significant imbalance.</p>
                    </div>
                </div>
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="image2.png" alt="Grouped Genre Distribution" />
                        <p class="caption">Figure 3: Balanced distribution after genre grouping.</p>
                    </div>
                </div>
            </div>
            
            <h3>Feature Engineering</h3>
            <p>Beyond structural reorganization, we engineered additional features to extract deeper insights from the existing data:</p>
            <ul>
                <li>Calculating sentiment polarity and subjectivity scores from movie titles and overviews</li>
                <li>Identifying sequels through pattern recognition in titles</li>
                <li>Counting genre-specific keywords in movie descriptions</li>
                <li>Deriving seasonal release information from dates</li>
            </ul>
            
            <p>Rigorous data cleaning procedures complemented these enhancements, including handling missing values in critical fields, eliminating duplicate entries, normalizing textual data to remove inconsistencies, and standardizing numerical features through scaling.</p>
        </section>
        
        <section id="methodology">
            <h2>3. Methodology</h2>
            
            <div class="model-comparison">
                <div class="model-card">
                    <h3>Text-Based Model</h3>
                    <p>Our text-based approach leveraged linguistic patterns in movie titles and overviews:</p>
                    <h4>Text Preprocessing:</h4>
                    <ul>
                        <li>Conversion to lowercase, removal of special characters and punctuation</li>
                        <li>Tokenization and filtering of English stop words</li>
                        <li>Stemming to reduce words to their root forms</li>
                    </ul>
                    <h4>Feature Engineering:</h4>
                    <ul>
                        <li>Sentiment polarity analysis of titles to capture emotional tone</li>
                        <li>Genre-specific keyword counting within overviews</li>
                        <li>Text normalization for consistent feature extraction</li>
                    </ul>
                    <h4>Model Development:</h4>
                    <ul>
                        <li>Genre frequency analysis to understand dataset composition</li>
                        <li>Creation of broader thematic groups based on individual genres</li>
                        <li>Standardization of numerical features for balanced model training</li>
                    </ul>
                </div>
                
                <div class="model-card">
                    <h3>Vision-Based Model</h3>
                    <p>Our vision approach focused on extracting genre information from movie poster imagery:</p>
                    <h4>Image Processing Pipeline:</h4>
                    <ul>
                        <li>Loading and validation of poster image paths</li>
                        <li>Resizing all images to 224×224 pixels</li>
                        <li>Tensor conversion and normalization for model compatibility</li>
                    </ul>
                    <h4>Model Architecture:</h4>
                    <ul>
                        <li>Implementation of vision transformer as the backbone feature extractor</li>
                        <li>Custom classification head for multi-label output (using Sigmoid activation)</li>
                        <li>PyTorch Dataset and DataLoader for efficient image handling</li>
                    </ul>
                    <h4>Training Strategy:</h4>
                    <ul>
                        <li>Incorporation of pre-calculated class weights to address genre imbalance</li>
                        <li>Binary Cross-Entropy Loss function optimization</li>
                        <li>Performance tracking using F1-score metrics</li>
                    </ul>
                </div>
                
                <div class="model-card">
                    <h3>Fusion Model</h3>
                    <p>Our trimodal fusion approach integrated text, images, and tabular data:</p>
                    <h4>Multimodal Integration:</h4>
                    <ul>
                        <li>Text pathway: BERT tokenizer and model for advanced text representation</li>
                        <li>Image pathway: EfficientNet for visual feature extraction</li>
                        <li>Tabular pathway: Multi-layer perceptron for processing statistical features</li>
                    </ul>
                    <h4>Advanced Training Techniques:</h4>
                    <ul>
                        <li>Implementation of either Focal Loss or weighted BCE loss with label smoothing</li>
                        <li>Gradient accumulation to simulate larger batch sizes</li>
                        <li>Early stopping based on validation performance metrics</li>
                    </ul>
                    <h4>Fusion Architecture:</h4>
                    <ul>
                        <li>Parallel processing of each modality through specialized pathways</li>
                        <li>Feature fusion layer to combine representations</li>
                        <li>Final classification layer optimized for multi-label prediction</li>
                    </ul>
                </div>
            </div>
            
          
        </section>
        
        <section id="results">
            <h2>4. Results & Evaluation</h2>
            <p>We evaluated each approach independently to assess their relative strengths:</p>
            
            <h3>Text-Based Model Results</h3>
            <p>Our BERT-based text classification model demonstrated significant and consistent improvement throughout the training process, achieving strong results for multi-label genre classification using only textual features. The training micro F1 score showed steady progression from approximately 0.63 at epoch 1 to 0.76 by epoch 10, while the validation micro F1 score improved from 0.67 to 0.73.</p>
            
            <div class="chart-row">
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="Basicsofai/image3.png" alt="Text Model Train Micro F1" />
                        <p class="caption">Figure 5: Training Micro F1 Score for Text Model</p>
                    </div>
                </div>
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="Basicsofai/image4.png" alt="Text Model Val Micro F1" />
                        <p class="caption">Figure 6: Validation Micro F1 Score for Text Model</p>
                    </div>
                </div>
            </div>
            
            <h3>Vision-Based Model Results</h3>
            <p>Our Vision Transformer (ViT) model for movie poster classification showed remarkable improvement over the training period. Starting from modest F1 scores around 0.2 at the first epoch, both training and validation metrics demonstrated steady and substantial gains, ultimately reaching impressive scores of approximately 0.8 by the seventh epoch.</p>
            
            <div class="chart-container">
                <img src="Basicsofai/image5.png" alt="Vision Model F1 Scores" />
                <p class="caption">Figure 7: F1 Scores per Epoch for the Vision Transformer Model</p>
            </div>
            
            <h3>Fusion Model Results</h3>
            <p>Our trimodal fusion approach, which integrated textual, visual, and tabular features, demonstrated exceptional performance characteristics. The loss curves show consistent improvement throughout the training period, with training loss decreasing from approximately 0.23 to 0.14 and validation loss reducing from 0.195 to 0.14.</p>
            
            <div class="chart-row">
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA7gAAAHiCAYAAAD0/k+bAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyRpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoTWFjaW50b3NoKSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDoyRkU3QUJEQzU5NzAxMUVGQTdDREMyMDAwNTM5QkRCQSIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDoyRkU3QURERDU5NzAxMUVGQTdDREMyMDAwNTM5QkRCQSI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOjJGRTdBQkRBNTk3MDExRUZBN0NEQzIwMDA1MzlCREJBIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjJGRTdBQkRCNTk3MDExRUZBN0NEQzIwMDA1MzlCREJBIi8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+FUgijAAAOHdJREFUeNrs3Ql4VNX9//FzZrLvIRAgJGwhCJKIyCIUQQuiqOCCxaX9uZRarEu1WhSp1rpVbeuCQqt9rFoVsYrWpQooCiKbgmwCsgTCEghLFrJnMsnM/M+5mQmEMJPMTDIJ5P16nknmLufee+Zkcr/3nnuOJssyAQAAAAAQaWJi2QQAAAAAACPN7Y+HH1Oqsb9D0Ps1r5MaazYDAAAAALw3bJkAAAAAgEBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOAS4AAAAAICAR4ALAAAAAAg4BLgAAAAAgIBDgAsAAAAACDgEuAAAAACAgEOACwAAAAAIOATQAAAAAICAww/VAAAAALRB9EEAAAAAAOjCCQAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMAhwAUAAAAABBwCXAAAAABAwCHABQAAAAAEHAJcAAAAAEDAIcAFAAAAAAQcAlwAAAAAQMCJYxMAAAAAADTaRNRymiRJDpeRzTynLCsXP4dg1jzlPZVfHe3q1rSK1GiTkfGGAABgI+rXgECh0+lsltX5m+Pc3vL06dMPGI1lNmvsmjVruiQmdNUvW7ZVr9frt3s6h09NmjSpMiS2udPbyTLV/Qg0Xv31w8LCEg12GbnDhw9Lbrdbbmxu3Gc1XXaTJMlV7rY72aBhvMGaM03qKNw+vVjWydpFXGNZJkuS7LRarXJZWdmUxMREB3+P0WnjgAcAwDZk1FUlJibrS0pKs3U63TqHTXM6NRpr62m6Vww9Rq6NtJBJiI8vCXWJDh48mJaUlLTPYrGk8OVGBwG8L3P8oZ3NZkvs1Kmow2gzGGI7TJs2TZfQubNWZmpDI5qYzWbd7t2754f6JJWVlQP37Nlzl9Vq7Ww0Gjvp9XotOzu7X/fu3T/PyMj4nWEYx9tjvS0Wi16v1zd5+23Zsm4kr41kMNpkzSU7TpjKs3VyYoJOO3ky/1SNZtPsN9/d4G+lzj6a3CnbUF5mTaVDJnQRiYnxpZIkVf4Ik6pMkkwJCQnyqFGj1vDZww8KAAA2IP/HdgNOG/oCeNq/sWX5m8fa6vL2AObXZVVVVXV0u912zRZV0TRJppgYo8R2RZDzv+/a6zvI7x9X/c9ykEkzmUwtHiS2hjbJkiStcdcrkz20Tq/X81lwEhUAoMV/ePqw3bb7c0f1sDpAeJOvtfJ5lOCnwTPa2Opk3VxUNEqDh4uzcnJzB1JMLJGmHd5jDleBRWdKi6Ou3bpU3Hjj9fUeBvnmP+vxPLe2vPaS1XLLLbfsMzRhXmEzL1+/8/+26s033zzf8Hdv4YDqnZf5W7rcttvY4o+4N3Xzx7mWZdnt9Wvmvs1tmKZtmRq6RLjm/bvxeeWaZW/8/2bVIeXkLHu03uf5nnWt9zk3vSxebLPGvvPePC/q8fW8oE7w6W+9ffm7+jXXghrY5u2sLtbm34TL9fzQU1x0Om3PnDmN1/+0z+I+bfHO/VUvlhvx19vbtrd5k/t9N+Y69v5+aq/3tZf3f+r9vOB/l5e3dzZv3p+ztdxE0dzBfyQGGPUdSCgvL5+ozE1oSNA0Pqw8PQjKZD6NPxdCPgB0j/cw9+KLx//y2Wefj7PbbRK1wyxJUgw7YLRRkiSWlZW1wB/9QpJlMo0cPfqLnJycxnRx97pJQ+P+t7Jz29ix2b9ISkriZa1lw9zszPscqtb64yqV9Vn46z+fOWGmF2XzR9bpZcDvMTnx81SV9PJi94W336DG9qFtblnZsJN30Pv+nOqPK1bWW0f+XkzNP6d+L90n3i8aU//6TawPS7Lkcrvl0ydXl4Ml9HVoqL65XV17aP6T8Kf5/b5dfn0/WdIcDkdjrnvVFV5cS5V1O
                    </div>
                </div>
                <div class="chart-item">
                    <div class="chart-container">
                        <img src="Basicsofai/image6.png" alt="Fusion Model F1 Scores" />
                        <p class="caption">Figure 9: F1 Scores over Epochs for Fusion Model</p>
                    </div>
                </div>
            </div>
            
            <div class="chart-container">
                <img src="/api/placeholder/900/300" alt="Model Performance Comparison" />
                <p class="caption">Figure 10: Performance Comparison of Text-Only, Image-Only, and Fusion Models</p>
            </div>
        </section>
        
        <section id="team">
            <h2>5. Team Contributions</h2>
            <table>
                <thead>
                    <tr>
                        <th>Name</th>
                        <th>Contributions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Mukesh Reddy</td>
                        <td>BERT, Vision Transformer, Fusion Model</td>
                    </tr>
                    <tr>
                        <td>Yashwant</td>
                        <td>Dataset Scraping, LLaMA Model Implementation, Website</td>
                    </tr>
                    <tr>
                        <td>Sreekar</td>
                        <td>Fusion Model, Dataset Scraping</td>
                    </tr>
                </tbody>
            </table>
        </section>
        
        <section id="future">
            <h2>6. Discussion & Future Work</h2>
            <p>Our multi-modal approach to movie genre prediction demonstrated promising results despite training constraints. The fusion model, which combined textual features from movie metadata with visual features extracted from poster images, showed improved performance over single-modality models.</p>
            
            <h3>Limitations</h3>
            <p>Due to computational limitations resulting in smaller batch sizes, the model's training period was necessarily abbreviated, potentially limiting its ability to fully capture complex relationships between visual and textual elements. The genre grouping strategy effectively addressed the initial class imbalance problem, though some nuance in genre-specific prediction capability was sacrificed in favor of more reliable overall performance across the consolidated categories.</p>
            
            <h3>Future Directions</h3>
            <p>Future work will focus on implementing EfficientNet as our core image feature extractor, which should provide better visual representation learning while requiring less computational resources. This efficiency improvement would allow for larger batch sizes and extended training periods, potentially enhancing the model's predictive accuracy.</p>
            
            <p>Additionally, we plan to develop a dedicated API that would enable real-time genre prediction for new movie releases, making our research findings accessible to industry stakeholders and content platforms. The API would incorporate both text and image analysis components, allowing users to submit movie metadata and poster images for immediate classification within our optimized genre grouping system.</p>
        </section>
        
        <section id="reproducibility">
            <h2>7. Code & Reproducibility</h2>
            <p>All code for this project is available in our GitHub repository. Follow these steps to reproduce our results:</p>
            
            <h3>Setup Instructions</h3>
            <ol>
                <li>Clone the repository: <code>git clone https://github.com/username/movie-genre-classification.git</code></li>
                <li>Install dependencies: <code>pip install -r requirements.txt</code></li>
                <li>Download the dataset using the provided scripts: <code>python scripts/download_data.py</code></li>
                <li>Run preprocessing: <code>python scripts/preprocess.py</code></li>
                <li>Train models individually: <code>python train_text.py</code>, <code>python train_vision.py</code>, or <code>python train_fusion.py</code></li>
                <li>Evaluate models: <code>python evaluate.py --model [text|vision|fusion]</code></li>
            </ol>
            
            <div class="code-block">
# Example code snippet for running the fusion model
import torch
from models.fusion import TrimodalFusionModel
from datasets import MovieDataset
from torch.utils.data import DataLoader

# Load model
model = TrimodalFusionModel(
    text_embedding_dim=768,
    image_embedding_dim=1280,
    tabular_features=15,
    num_classes=5
)

# Load dataset
test_dataset = MovieDataset('data/test', transform=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Load weights
model.load_state_dict(torch.load('checkpoints/fusion_model.pth'))
model.eval()

# Run evaluation
evaluate(model, test_loader)
            </div>
        </section>
    </div>
    
    <footer>
        <div class="container">
            <p>© 2025 Multi-Modal Movie Genre Classification Project | EAS 510LEC-AI1 (23550), Basics of AI</p>
            <p>University at Buffalo, Spring 2025</p>
        </div>
    </footer>
</body>
</html>